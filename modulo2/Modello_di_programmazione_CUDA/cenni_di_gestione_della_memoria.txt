MODELLO DI MEMORIA CUDA
    • Il modello CUDA presuppone un sistema con un host e un device, ognuno con la propria memoria.
    • La comunicazione tra la memoria dell'host e quella del device avviene tramite il bus seriale PCIe (Peripheral Component
      Interconnect Express), che permette di trasferire dati tra CPU e GPU.
CARATTERISTICHE PCIe
    • Lane: Ogni lane (canale di trasmissione) è costituito da due coppie di segnali differenziali (quattro fili), una per ricevere
      e una per trasmettere dati.
    • Full-Duplex: Trasmette e riceve dati simultaneamente in entrambe le direzioni.
    • Scalabilità: La larghezza di banda varia a seconda del numero di lane: x1, x2, x4, x8, x16.
    • Bassa Latenza: Garantisce comunicazioni rapide e reattive nei trasferimenti frequenti.
    • Collo di Bottiglia: Può diventare un collo di bottiglia in trasferimenti di grandi volumi tra CPU e GPU.

MODELLO DI MEMORIA CUDA 2
    • I kernel CUDA operano sulla memoria del device.
    • CUDA Runtime fornisce funzioni per:
        ○ Allocare memoria sul device.
        ○ Rilasciare memoria sul device quando non più necessaria.
        ○ Trasferire dati bidirezionalmente tra la memoria dell'host e quella del device

Standard C      CUDA C          Funzione
malloc          cudaMalloc      Alloca memoria dinamica
memcpy          cudaMemcpy      Copia dati tra aree di memoria
memset          cudaMemset      Inizializza memoria a un valore specifico
free            cudaFree        Libera memoria allocata dinamicamente

NB: le operazioni CUDA C agiscono sulla memoria globale della GPU

NB_2: È responsabilità del programmatore gestire correttamente l'allocazione, il trasferimento e la deallocazione della memoria
per ottimizzare le prestazioni.

NB_3: non c'è modo di distinguere puntatori a memoria host rispetto a memoria device.

--- GERARCHIE DI MEMORIA
In CUDA, esistono diversi tipi di memoria, ciascuno con caratteristiche specifiche in termini di accesso, velocità, e visibilità.
Per ora, ci concentriamo su due delle più importanti:
    
Global memory   
    • Accessibile da tutti i thread su tutti i blocchi
    • Più grande ma più lenta rispetto alla shared memory
    • Persiste per tutta la durata del programma CUDA
    • È adatta per memorizzare dati grandi e persistenti
Shared Memory
    • Condivisa tra i thread all'interno di un singolo blocco
    • Più veloce, ma limitata in dimensioni
    • Esiste solo per la durata del blocco di thread
    • Utilizzata per dati temporanei e intermedi

NB: le funzioni
    • cudaMalloc: Alloca memoria sulla GPU.
    • cudaMemcpy: Trasferisce dati tra host e device.
    • cudaMemset: Inizializza la memoria del device.
    • cudaFree: Libera la memoria allocata sul device.
operano principalmente sulla GLOBAL Memory.




// chiedi a chat-gpt  per "const" nel parametro della cuda_memCpy

